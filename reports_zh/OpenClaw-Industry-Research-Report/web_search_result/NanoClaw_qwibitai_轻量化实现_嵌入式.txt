Search Query: NanoClaw qwibitai 轻量化实现 嵌入式
Search Time: 2026-02-11 06:47:28
Total Results: 5
================================================================================

[1] 一种用于嵌入式系统的深层聚合神经网络轻量化方法（发布时间：2022-03-05 01:02:00）
Source: X技术网
URL: https://www.xjishu.com/zhuanli/55/202111497896.html
Summary: 1.本发明涉及深度学习技术领域，尤其涉及一种用于嵌入式系统的深层聚合神经网络轻量化方法。背景技术：2.视觉识别任务需要丰富的信息，信息丰富程度从低到高，尺度从小到大。随着卷积神经网络的深度变化，单独的一层并不能够提供足够的信息，只有聚合这些信息，才能够提高获取是什么与在哪里信息的准确性。现有的很多致力于网络结构设计的工作，包括探索不同的网络结构，设计更深、更大的网络。在2019年有人提出深层聚合的网络结构，它更好地融合网络不同层、块之间的信息，能够迭代式地将网络结构特征信息融合起来，为多尺度目标识别提供了解决方案。3.为了将深度卷积神经网络模型应用于实时应用和低内存的嵌入式设备，一个可行的解决方案是对深度神经网络进行压缩和加速，以减少参数、计算成本和功耗。例如mobilenet是一个轻量级的网络，它使用深度可分离的卷积来加深网络以达到减少参数和计算量。同时mobilenet在imagenet数据集上的分类准确率仅降低了1％。但是，目前的轻量型网络没有对网络层进行探究以期适应多尺度识别，相比之下深层聚合的网络识别准确度更好，而目前还没有对深层聚合网络轻量化的方法。4.中国发明cn201910145961.4”主要使用mobilenet-ssd模型对手势进行检测，它主要是通过修改ssd中的vgg16部分，用mobilenet网络中的深度可分离卷积去替换vgg16中的标准卷积，以达到模型轻量化的目的。中国发明cn201810745187.6”使用mobilenet-v2模型对目标进行识别，可以在嵌入式系统中实时运行。5.中国专利cn201910
--------------------------------------------------------------------------------

[2] 基于多目标优化的轻量化深度学习技术研究-知网阅读
URL: https://read.cnki.net/web/Dissertation/Article/10701-1022014361.nh.html
Summary: 知网阅读 App
阅读改变世界
打 开
基于多目标优化的轻量化深度学习技术研究
西安电子科技大学
马向前
深度神经网络的轻量化是其在边缘嵌入式设备实现的必要步骤,主要包括参数量化、模型剪枝、轻量卷积核设计、知识蒸馏等几类方法。现有方法中,模型轻量化(或模型压缩)大多被抽象为给定资源约束下的单目标优化问题,如性能约束、参数约束等。然而,深度神经网络现实应用中受到性能、计算、存储等多方面因素的限制,因此模型轻量化天然具备多目标优化的特点。现有方法由于未将计算资源、存储资源等建模为独立的多个优化目标,算法每一次运行仅可给出单个压缩模型,无法满足不同场景的差异化需求。相比于单目标优化算法,多目标优化算法可在单次求解中同时优化多个目标,并给出具有不同表现的多个Pareto最优解。基于上述分析,本文在模型剪枝框架下,建立模型轻量化的多目标评估体系,将模型轻量化问题建模为模型性能、计算资源和存储资源等多个目标引导下的优化问题,并借助人工蜂群算法、知识蒸馏、多目标免疫算法等方法求解以获得基于多目标优化的轻量化深度学习算法。具体研究内容如下:(1)设计了基于多目标聚合函数的人工蜂群剪枝算法。将模型剪枝建模为多目标聚合函数引导下的子结构搜索问题:首先将子结构进行蜜源编码,设计子结构参数共享训练算法准确度量子结构性能;接着定义分类准确率、浮点运算数和参数量三个目标,并设计了线性加权、负对数和指数衰减三种聚合函数,将三个目标聚合为一个以引导人工蜂群算法的搜索方向;最后使用编码修改个数随机生成、互补编码初始化和最优蜜源引导三个策略,提升人工蜂群算法的搜索能力,解决了基于重要性
--------------------------------------------------------------------------------

[3] 2024年南网数研院嵌入式关键技术、方法和轻量化设计项目采购二次招标招标公告
Source: bidnews.cn
URL: https://www.bidnews.cn/caigou/zhaobiao-74615430.html
Summary: 1.1.项目名称：20
南网数研院嵌入式关键技术、方法和轻量化设计项目采购（二次招标）
1.2.招标编号：CG2
1.3.招标人：南方电网数字电网研究院股份有限公司
1.4.采购方式：公开招标
1.5.招标分类：专项
1.6.项目类别：服务
1.7.资金来源：已落实
1.8.资格审查方式：资格后审
二、项目概况和招标范围
2.1项目概述：
本项目进行低压配网控制芯片、控制单元的通信方法研究。进一步地，基于设计的芯片和控制单元的算力资源，在嵌入式装置上实现轻量化的人工智能应用，通过微模型构建的关键技术解决工控装备的计算性能瓶颈，将大量复杂计算与AI处理工作下沉到离业务终端更近的边缘平台。南方电网数字电网研究院股份有限公司为合同签订的主体。
2.2招标范围：
开展电力人工智能算法轻量化设计及深度压缩技术研究，协助招标方完成电力人工智能算法轻量化设计及深度压缩技术相关报告、专利、论文初稿等。
2.3标的清单及分包情况如下：
序号 标的编码 标的 概算金额（万元） 最大中标数量 标包编码 标包号 标包名称 标包金额（万元） 最高限价（万元） 限价说明 服务期/工期
1 CG2100022001746181001 电力人工智能算法轻量化设计及深度压缩技术研究 110 1 CG2100022001746181001001 1 电力人工智能算法轻量化设计及深度压缩技术研究 110 110 详见报价表 合同签订之日起至202
备注：（1）除不受限制的特殊包外，一个投标人在一个标的中最多只能中不超过最大中标包数的标包。
注：本项目不收取招标文件费用及投标保证金。
三、投标人资格要求
通用资格要求
--------------------------------------------------------------------------------

[4] 超轻量深度学习模型：如何在嵌入式设备上运行 Transformer（发布时间：2025-02-17 10:17:43）
Source: CSDN博客
URL: https://blog.csdn.net/m0_69441654/article/details/145676260
Summary: 1. 引言
随着 AI 技术的发展，Transformer 模型已经在自然语言处理（NLP）和计算机视觉（CV）等领域取得了巨大成功。然而，Transformer 计算量大，参数众多，难以直接部署到嵌入式设备（如树莓派、Jetson Nano、Edge TPU）。
本教程将详细介绍 如何优化 Transformer 模型，使其能高效运行在资源受限的设备上。我们将探讨：
Transformer 模型的计算瓶颈
轻量化方法（模型剪枝、量化、蒸馏）
在嵌入式设备上的部署方案
代码实现与优化策略
2. Transformer 模型的计算挑战
Transformer 由于使用 多头自注意力机制（Multi-Head Self Attention, MHSA），计算复杂度为 O(n2)，使其在嵌入式设备上难以高效运行。
2.1 计算瓶颈
Transformer 的计算主要集中在：
Self-Attention 计算复杂度高
模型参数量大，占用内存多
推理时计算延迟高
例如，BERT-base 具有 110M 参数，计算量超过 10 GFLOPs，对于嵌入式设备来说负担过重。
3. 轻量化 Transformer 方法
3.1 模型剪枝（Pruning）
模型剪枝通过去除不重要的权重或神经元来减少计算量。
代码示例：使用 Hugging Face 进行剪枝
from transformers import DistilBertModel
import torch.nn.utils.prune as prune
model DistilBertModel.frompretrained("dist
--------------------------------------------------------------------------------

[5] 知识蒸馏、轻量化模型架构、剪枝…几种深度学习模型压缩方法（发布时间：2023-03-13 15:26:00）
Source: 博客园
URL: https://www.cnblogs.com/huaweiyun/p/17211587.html
Summary: 摘要：模型压缩算法旨在将一个大模型转化为一个精简的小模型。工业界的模型压缩方法有：知识蒸馏、轻量化模型架构、剪枝、量化。
本文分享自华为云社区《深度学习模型压缩方法综述》，作者：嵌入式视觉 。
一，模型压缩技术概述
因为嵌入式设备的算力和内存有限，因此深度学习模型需要经过模型压缩后，方才能部署到嵌入式设备上。
在一定程度上，网络越深，参数越多，模型也会越复杂，但其最终效果也越好。而模型压缩算法是旨在将一个庞大而复杂的预训练模型转化为一个精简的小模型。本文介绍了卷积神经网络常见的几种压缩方法。
按照压缩过程对网络结构的破坏程度，《解析卷积神经网络》一书中将模型压缩技术分为前端压缩”和后端压缩”两部分:
前端压缩，是指在不改变原网络结构的压缩技术，主要包括
知识蒸馏
、轻量级网络（紧凑的模型结构设计）以及
滤波器（filter）层面的剪枝（结构化剪枝）
等；
后端压缩，是指包括
低秩近似
、未加限制的剪枝（非结构化剪枝/稀疏）、
参数量化
以及二值网络等，目标在于尽可能减少模型大小，会对原始网络结构造成极大程度的改造。
总结：前端压缩几乎不改变原有网络结构（仅仅只是在原模型基础上减少了网络的层数或者滤波器个数），后端压缩对网络结构有不可逆的大幅度改变，造成原有深度学习库、甚至硬件设备不兼容改变之后的网络。其维护成本很高。
1.1，模型压缩技术分类
工业界主流的模型压缩方法有：知识蒸馏（Knowledge Distillation，KD）轻量化模型架构（也叫紧凑的模型设计）、剪枝（Pruning）、量化（Quantization）。各个模型压缩方法总结如下：
模型压缩方法
描述
涉及的
--------------------------------------------------------------------------------
